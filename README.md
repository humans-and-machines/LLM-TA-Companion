# LLMs as TA Companions
## Project Overview
Teaching assistants (TAs) are essential to student learning, providing grading and feedback, yet these processes remain time-intensive and often inconsistent, especially in large proof-based courses.
At the same time, Large Language Models (LLMs) have attracted interest in educational contexts, although their limitations often hinder their use in scenarios like grading.
Here, we ask: *can language models assist TAs in complex, proof-based courses?*

We conduct three pre-registered studies, approved by our Institutional Review Board, that apply learning analytics methods to replicate and extend prior findings on AI-augmented grading and feedback provision to the underexplored context of proof education. 
Unlike prior work, we examine a "TA-in-the-loop" structure, where TAs can revise LLM-generated feedback before it is sent to students. 
Our studies measure the accuracy and variability in grading across LLMs and TAs (Studies #1 and #2) and the extent to which feedback generated by LLMs can be helpful to TAs (Study \#3).


Our findings indicate that LLMs are effective at identifying errors in student solutions but exhibit considerable variability in rubric item assignment relative to human graders.
Further, although the LLM-produced responses were often verbose, TAs generally viewed them as valuable and helpful, especially for submissions containing significant errors.
Overall, our results indicate that while LLMs are not yet completely reliable for grading, they show potential in providing scalable, structured feedback to improve the quality of student solutions.
More broadly, by situating our work in proof education, we advance understanding of how AI-augmented grading and feedback provision generalize to new domains and inform the design of end-to-end systems that integrate LLMs and human oversight in higher education.


## Repository Overview
In this repository, we provide [examples](/examples/) of problem statements and grading rubrics used in the proof-based course investigated in our work, the [prompts](/prompts/) used in our studies, and the [figures](/figures/) used in our paper. 